{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MQKVpdoYWla"
      },
      "outputs": [],
      "source": [
        "# Import all libraries\n",
        "# Make sure to run this first (or anytime the session times out)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1yKoT05ZJOn"
      },
      "outputs": [],
      "source": [
        "# Relative paths for both csv files\n",
        "path_fer2013 = 'fer2013.csv'\n",
        "path_phoebe = 'phoebe_AU.csv'\n",
        "\n",
        "# Store the databases\n",
        "data_fer = pd.read_csv(path_fer2013)\n",
        "data_phoebe = pd.read_csv(path_phoebe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX22QRcX_CLa"
      },
      "source": [
        "# Part A - Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nje8abyQ_NRp"
      },
      "source": [
        "## 1. fer2013.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjO701JyjaAl"
      },
      "outputs": [],
      "source": [
        "# Get the training data\n",
        "\n",
        "# Make sure to shape to 48x48 and loop through each image and seperate by space\n",
        "x_train = np.array([np.fromstring(image, dtype=int, sep=' ').reshape(48, 48) for image in data_fer[data_fer['Usage'] == 'Training']['pixels']])\n",
        "y_train = np.array(data_fer[data_fer['Usage'] == 'Training']['emotion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "lIfb4IxKRVmJ",
        "outputId": "71e815a1-4b81-46f2-f790-bca42f9db476"
      },
      "outputs": [],
      "source": [
        "# Train a SVM model\n",
        "svm_fer = SVC()\n",
        "svm_fer.fit(x_train.reshape(len(x_train), -1), y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkKEaWijlhNV"
      },
      "outputs": [],
      "source": [
        "# Read and processs private test data\n",
        "x_privatetest = np.array([np.fromstring(image, dtype=int, sep=' ').reshape(48, 48) for image in data_fer[data_fer['Usage'] == 'PrivateTest']['pixels']])\n",
        "y_privatetest = np.array(data_fer[data_fer['Usage'] == 'PrivateTest']['emotion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L2laL7tltX2"
      },
      "outputs": [],
      "source": [
        "# Predict emotions using SVM\n",
        "y_pred_a1 = svm_fer.predict(x_privatetest.reshape(len(x_privatetest), -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KwbX2DxqHeP",
        "outputId": "9975c7b4-1a15-43da-f922-f97876100481"
      },
      "outputs": [],
      "source": [
        "# Look at the classication report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_privatetest, y_pred_a1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "iZ_xmCFaqRFR",
        "outputId": "7f97143e-4816-4d47-f794-281cb1b9d7ff"
      },
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "conf_matrix_a1 = confusion_matrix(y_privatetest, y_pred_a1)\n",
        "\n",
        "# Plot it\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix_a1, annot=True, cmap='Blues', fmt='g')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L688alufji_"
      },
      "source": [
        "\n",
        "As shown in the classification report above, group 1 has a perfect precision (1.0), meaning it has no false positives. So when the classifier predicts group 1, it is always correct. However, despite this, group 1 has the lowest f1-score (0.10) out of all the groups because the recall is really low. The classifier does not predict group 1 a lot, leading to a lot of false negatives.\n",
        "\n",
        "Groups 3 and 5 have the highest f1-scores (0.58) indicating they are the best, by some margin, at balancing between precision and recall.\n",
        "\n",
        "The total accuracy of the classifier is 0.45, indicating the classifier correctly predicts the emotional label 45% of the time. The macro average is the average score treating all classes equally and the weighted average takes the size of groups into account. Since the weight average is greater than the macro average, we can say the larger sized groups have a higher f1-score or there are some small outlier groups with a low f1-score. Looking at the data we can see the outlier is group 1 as it has by far the smallest f1-score (0.1) and size (55) compared to the other 6 groups.\n",
        "\n",
        "The confusion matrix can be seen above. Each group other than group 1 has the largest value in their correctly predicted cell. Groups 3, 4, 5, and 6 have the largest number of predicted labels correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNKfmTBZC7pH"
      },
      "outputs": [],
      "source": [
        "# Paths for images\n",
        "image_one = 'images/unknown/1_01.jpg'\n",
        "image_two = 'images/unknown/4_01.jpg'\n",
        "image_three = 'images/unknown/4_20.jpg'\n",
        "image_four = 'images/unknown/8_01.jpg'\n",
        "image_five = 'images/unknown/9_41.jpg'\n",
        "image_six = 'images/unknown/26_123.jpg'\n",
        "image_seven = 'images/unknown/35_42.jpg'\n",
        "image_eight = 'images/unknown/41_06.jpg'\n",
        "image_nine = 'images/unknown/44_01.jpg'\n",
        "image_ten = 'images/unknown/46_03.jpg'\n",
        "image_eleven = 'images/unknown/48_01.jpg'\n",
        "image_twelve = 'images/unknown/52_31.jpg'\n",
        "\n",
        "# Store paths in an array\n",
        "image_paths = [image_one, image_two, image_three, image_four, image_five,\n",
        "               image_six, image_seven, image_eight, image_nine, image_ten,\n",
        "               image_eleven, image_twelve]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "CURFxZVFC-ct",
        "outputId": "fb67b844-3498-49fb-f273-2d9f9d11f318"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store processed images\n",
        "processed_images = []\n",
        "\n",
        "# Loop over each image path\n",
        "for image_path in image_paths:\n",
        "    # Read image, turn to gray, and resize\n",
        "    image = cv2.imread(image_path)\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    resized_image = cv2.resize(gray_image, (48, 48))\n",
        "\n",
        "    # Flatten image to create feature vector\n",
        "    flattened_image = resized_image.flatten()\n",
        "\n",
        "    # Move into array\n",
        "    processed_images.append(flattened_image)\n",
        "\n",
        "# Convert the list to a numpy array\n",
        "X_phoebe_unknown = np.array(processed_images)\n",
        "\n",
        "# Predict emotions for the Phoebe unknown dataset\n",
        "y_phoebe_unknown_pred = svm_fer.predict(X_phoebe_unknown)\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Predicted emotions for Phoebe unknown dataset:\", y_phoebe_unknown_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2F1ISXYth51"
      },
      "source": [
        "The model only predicted either emotional label 3 or 4 for all 12 of the unknown images. Label 3 represents happy and label 4 represents sad. Looking at the images, it seems the model predicted happy for all images where Phoebe shows her teeth and sad when she does not (outside of the first two images: 1_01 and 4_01). Overall I would say the model correctly predicted 5 of the images correctly. Those images being 4_20 (sad), 8_01 (happy), 9_41 (happy), 41_06 (happy), and 46_03 (happy). This gives us an average of 5/12 = 0.417, which is similar to the accuracy and average scores from the model's classification report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ue4VgD18w8I"
      },
      "source": [
        "## 2. SVM Using Action Units"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0vzTK2NoVrd"
      },
      "outputs": [],
      "source": [
        "# Get rid of all the unknown labels\n",
        "filtered_data = data_phoebe[data_phoebe['label'] != 'unknown']\n",
        "\n",
        "# Seperate the AU columns and label column\n",
        "X_phoebe = filtered_data.drop(columns=['label', 'file_name'])\n",
        "y_phoebe = filtered_data['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVsVvqudofil"
      },
      "outputs": [],
      "source": [
        "# Initialize the SVM classifier\n",
        "svm_phoebe = SVC()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNCRXE6aojc4"
      },
      "outputs": [],
      "source": [
        "# 5-fold cross-validation\n",
        "cv_scores = cross_val_score(svm_phoebe, X_phoebe, y_phoebe, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpw78PlAooBL",
        "outputId": "312ac36f-4d6d-4d71-b79b-55bf143a05c7"
      },
      "outputs": [],
      "source": [
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean accuracy:\", cv_scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG0RygKqv89f"
      },
      "source": [
        "The cross validation scores show the different accuracy scores achieved by the model on different folds of the dataset. The highest accuracy came from the second fold and the lowest came from the third fold. The mean accuracy of the 5 folds was 0.5. This represents an estimation of the model's ability to perform on unseen data. So we can expect this model to be about 50% right at predicting the emotions of the unknown images of Phoebe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "1saTHDiwopAh",
        "outputId": "8d97ab29-f719-4368-d7a9-e1c00e41568d"
      },
      "outputs": [],
      "source": [
        "# Train the SVM model\n",
        "svm_phoebe.fit(X_phoebe, y_phoebe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owj48B_ootUR",
        "outputId": "60a44525-153d-4440-d87c-f74818e33771"
      },
      "outputs": [],
      "source": [
        "# Predict emotions for all 12 unknown samples\n",
        "unknown_samples = data_phoebe[data_phoebe['label'] == 'unknown']\n",
        "X_unknown = unknown_samples.drop(columns=['label', 'file_name'])\n",
        "y_unknown_pred = svm_phoebe.predict(X_unknown)\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predicted emotions for unknown samples:\")\n",
        "print(y_unknown_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM8L7fvLylkD"
      },
      "source": [
        "As shown above, the model predicts the emotions of the 12 images as surprise, surprise, sad, happy, happy, sad, sad, happy, surprise, happy, surprise, and surprise. These images are in order of file_name (the order they are in the csv file). For example, the first emotion label is for image 1_01 and the second is for 4_01, and so on.\n",
        "\n",
        "I would label the 12 unknown images (using only the labels used in the csv file) as: 1_01=sad, 4_01=angry, 4_20=sad, 8_01=happy, 9_41=happy, 26_123=angry, 35_42=surprise, 41_06=happy, 44_01=angry, 46_03=happy, 48_01=disgusted, and 52_31=surprise (these images shown in cell below).\n",
        "\n",
        "Comparing the model's prediction with my own labels of the unknown images, I would say the model got 6/12 (50%) correct. The model was able to correctly predict happy all four times, with perfect precision and recall (f1-score=1). It also predicted sad and surprise correctly 1 time each. The model did not predict either of anger or disgust one time. It predicted correctly 50% of the images, which is exactly what the model predicted the accuracy would be for unseen. However, it is import to remember the unknown dataset is very small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for img_path in image_paths:\n",
        "    print(img_path)\n",
        "    img = mpimg.imread(img_path)\n",
        "    imgplot = plt.imshow(img)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqrcxUFksOdm"
      },
      "source": [
        "# Part B - Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-98Hs_P39oCY"
      },
      "source": [
        "## 1. Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrinq1zfCKrw"
      },
      "outputs": [],
      "source": [
        "# Convert pixel values to numpy arrays and normalize\n",
        "X = np.array([np.fromstring(image, dtype=int, sep=' ').reshape(48, 48, 1) for image in data_fer['pixels']])\n",
        "X = X / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECk0D1SEC4aD"
      },
      "outputs": [],
      "source": [
        "# Convert emotion labels to categorical\n",
        "y = to_categorical(data_fer['emotion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opfPWKzNC6mq"
      },
      "outputs": [],
      "source": [
        "# Split the data into training, public test and private test\n",
        "X_train_b = X[data_fer['Usage'] == 'Training']\n",
        "y_train_b = y[data_fer['Usage'] == 'Training']\n",
        "\n",
        "X_public = X[data_fer['Usage'] == 'PublicTest']\n",
        "y_public = y[data_fer['Usage'] == 'PublicTest']\n",
        "\n",
        "X_private = X[data_fer['Usage'] == 'PrivateTest']\n",
        "y_private = y[data_fer['Usage'] == 'PrivateTest']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b8EE6rvC9kN"
      },
      "outputs": [],
      "source": [
        "# Create the nn model\n",
        "def create_nn_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(7, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model_nn = create_nn_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLCX1X3cDEYh"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model_nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkjXgi3thZK0"
      },
      "outputs": [],
      "source": [
        "# Data augmentation to help with overfitting\n",
        "over = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True)\n",
        "over.fit(X_train_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJFTVFrihb9b",
        "outputId": "3a155ac9-4a72-4db0-ea22-b185e7c82583"
      },
      "outputs": [],
      "source": [
        "# Testing different batch size and epoch values\n",
        "# Batch size = 32 and epochs = 6\n",
        "# Takes about 15 mins to run on my machine\n",
        "history_nn1 = model_nn.fit(over.flow(X_train_b, y_train_b, batch_size=32), epochs=6, validation_data=(X_public, y_public))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZZExM5EjCZH",
        "outputId": "5357eb77-bb2b-4635-9cc8-0133e165545b"
      },
      "outputs": [],
      "source": [
        "model_nn = create_nn_model()\n",
        "model_nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Batch size = 64 and epochs = 5\n",
        "# Takes about 15 mins to run on my machine\n",
        "history_nn2 = model_nn.fit(over.flow(X_train_b, y_train_b, batch_size=64), epochs=5, validation_data=(X_public, y_public))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_zEbVMVlPLu",
        "outputId": "8a4d0c1c-3174-46ba-f071-0a10538a271f"
      },
      "outputs": [],
      "source": [
        "model_nn = create_nn_model()\n",
        "model_nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Batch size = 10 and epoch = 8\n",
        "# Takes about 15 mins to run on my machine\n",
        "history_nn3 = model_nn.fit(over.flow(X_train_b, y_train_b, batch_size=10), epochs=8, validation_data=(X_public, y_public))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7lcvy4BxDzW"
      },
      "source": [
        "After testing 3 different batch sizes and epochs numerous times, batch size = 10 and epoch = 8 consistently performs the best compared to the other variations. I will stick with these parameters for the next parts of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVwHm0cLDg54",
        "outputId": "df21b40b-0666-456c-8a06-ca2845c622f9"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model and calculate performance\n",
        "y_pred = np.argmax(model_nn.predict(X_private), axis=1)\n",
        "y_true = np.argmax(y_private, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "report = classification_report(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSavERKHSGId",
        "outputId": "e80477ab-4c55-41c5-ff5b-c434ff539662"
      },
      "outputs": [],
      "source": [
        "# Print performance metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "0nwWisscTSAj",
        "outputId": "66c3a176-61b5-40ff-df84-41738f17667e"
      },
      "outputs": [],
      "source": [
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXuIhhsv5hE9"
      },
      "source": [
        "The classification report and confusion matrix are shown above. These values will be different every single time due to the randomness or variation of the model that is being run. The below writing is the results from the last time I ran this model (almost all results are usually pretty similar to these).\n",
        "\n",
        "Looking at the classification report, group 3 performed the best with precision, recall and f1-scores of 0.74, 0.81, and 0.77 respectively. Group 5 also performed much better than the other groups with a f1-score of 0.66.\n",
        "\n",
        "One thing that stands out is group 1 having a f1-score of 0.00. Looking at the confusion matrix, we can see the model never predicted group 1. This is similar to Part A.1, where group 1 is not being predicted very much relative to the other 6 groups. Also, similar to Part A.1, group 3 sill has by far the most true positive.\n",
        "\n",
        "The accuracy of the model is 0.54 with the macro average being 0.43 and weighted average being 0.52. This discrepancy between macro and weighted makes sense since group 1 is really small and has a very low score, while a lot of the bigger groups have higher scores. This trend is consistently true every time I run this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8DlUoAb9zlU"
      },
      "source": [
        "## 2. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib6IIFH5DjQv"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    resized_image = cv2.resize(gray_image, (48, 48))\n",
        "    normalized_image = resized_image / 255.0\n",
        "    preprocessed_image = normalized_image.reshape(1, 48, 48, 1)\n",
        "    return preprocessed_image\n",
        "\n",
        "# Classify image using the model\n",
        "def classify_image(image_path, model):\n",
        "    preprocessed_image = preprocess_image(image_path)\n",
        "    prediction = model_nn.predict(preprocessed_image)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "    return predicted_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7gAfKxXf52B",
        "outputId": "14a3ff0c-5b5e-465c-f79c-aec59edd9bb9"
      },
      "outputs": [],
      "source": [
        "# Predict each unknown image with neural network model\n",
        "print(\"0=angry, 1=disgust, 2=fear, 3=happy, 4=sad, 5=surprise, 6=neutral\")\n",
        "\n",
        "for img in image_paths:\n",
        "  predicted_class = classify_image(img, model_nn)\n",
        "  print(f\"Predicted class for {img}: {predicted_class}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KOCbjH18cf1"
      },
      "source": [
        "The predicted emotion for each image is printed above. The results of these predictions will be slightly different every time the neural network model is run. However, most of the time there are not too many big fluctuations in the results.\n",
        "\n",
        "When writing this, the model had a accuracy of about 54% and based on the results it was able to predict the unknown Phoebe images at a similar rate. It was able to correctly predict 6/12 images (4_20, 8_01, 9_41, 35_42, 41_06, and 46_03). It also predicted neutral for images 1_01 and 52_31. Even though the Phoebe dataset does not have neutral in it, I could argue both those images could be interpreted as neutral.\n",
        "\n",
        "Comparing with the SVM model from A.2, both models were able to correctly predict the labels at about the same rate. Both models were very good at predicting happy and sad, but did not predict anger or disgust much. Overall, both models had similar accuracy scores and showed that with their predictions of the images, however, just based of looking at the images, I would prefer the Neural Network model over the SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb7_YM0N-JYI"
      },
      "source": [
        "## 3. Fine-Tune the Neural Network and Re-Classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzSvZTh8E0jB"
      },
      "outputs": [],
      "source": [
        "# Store the paths of every image into an array\n",
        "\n",
        "# Directory we are starting from\n",
        "base_dir = Path(\"images\")\n",
        "\n",
        "image_paths2 = []\n",
        "\n",
        "# Iterate through all subdirectories in the base directory\n",
        "for emotion_dir in base_dir.glob(\"*\"):\n",
        "    # Check if the subdirectory is a directory and do not include unknown images\n",
        "    if emotion_dir.is_dir() and emotion_dir.name != \"unknown\":\n",
        "        # Iterate through all image files in the subdirectory\n",
        "        for image_file in emotion_dir.glob(\"*.jpg\"): \n",
        "            # Add the relative path of the image file to the list\n",
        "            relative_path = \"images/\" + emotion_dir.name + \"/\" + image_file.name\n",
        "            # Add the path to our paths array\n",
        "            image_paths2.append(relative_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYWza2Q_KqWM"
      },
      "outputs": [],
      "source": [
        "# Create array to store emotional labels in order\n",
        "labels_known = []\n",
        "\n",
        "for image_path in image_paths2:\n",
        "    # Extract label from image path\n",
        "    label = os.path.basename(os.path.dirname(image_path))\n",
        "    # Append label to array\n",
        "    labels_known.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52vARBgYWgvw"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess each image\n",
        "def preprocess_image2(image_path, target_size=(48, 48)):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize(target_size)\n",
        "    # Convert the image to grayscale\n",
        "    image = image.convert('L')\n",
        "    image_array = np.array(image).reshape((*target_size, 1))\n",
        "    image_array = image_array / 255.0\n",
        "    return image_array\n",
        "\n",
        "# Preprocess every image with an emotional label\n",
        "X_ft = np.array([preprocess_image2(image_path) for image_path in image_paths2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnTePmuBW7ri"
      },
      "outputs": [],
      "source": [
        "# Turn the emotions into number so we can categorize them\n",
        "# 0=angry, 1=disgust, 2=fear, 3=happy, 4=sad, 5=surprise, 6=neutral\n",
        "y_ft = []\n",
        "\n",
        "for i in labels_known:\n",
        "  if i == 'angry':\n",
        "    y_ft.append('0')\n",
        "  elif i == 'disgust':\n",
        "    y_ft.append('1')\n",
        "  elif i == 'fear':\n",
        "    y_ft.append('2')\n",
        "  elif i == 'happy':\n",
        "    y_ft.append('3')\n",
        "  elif i == 'sad':\n",
        "    y_ft.append('4')\n",
        "  elif i == 'surprise':\n",
        "    y_ft.append('5')\n",
        "  elif i == 'neutral':\n",
        "    y_ft.append('6')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksl2g0IIYMLc"
      },
      "outputs": [],
      "source": [
        "y_phoebe_ft = to_categorical(y_ft, num_classes=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQXoLtV_SbBX",
        "outputId": "37374a50-97ba-4749-bef6-417dd69c8510"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_ft, y_phoebe_ft, test_size=0.2, random_state=42)\n",
        "\n",
        "# Freeze layers\n",
        "for layer in model_nn.layers[:4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile with different learning rate\n",
        "model_nn.compile(optimizer=Adam(learning_rate=0.002), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Increase the epochs since it will run much faster (20 is more than enough since it converges quickly)\n",
        "history = model_nn.fit(X_train, y_train, batch_size=10, epochs=20, validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6_2pK5ypOiP",
        "outputId": "6f840c1e-c464-4fc7-eb12-af6a37e0c5a1"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "y_pred2 = np.argmax(model_nn.predict(X_val), axis=1)\n",
        "y_true2 = np.argmax(y_val, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzl1EEwOpZEm",
        "outputId": "c04c9344-dc73-49da-f495-93d5b9f6edf2"
      },
      "outputs": [],
      "source": [
        "# Calculate performance metrics\n",
        "accuracy2 = accuracy_score(y_true2, y_pred2)\n",
        "report2 = classification_report(y_true2, y_pred2)\n",
        "\n",
        "print(\"Accuracy:\", accuracy2)\n",
        "print(\"Classification Report:\\n\", report2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "N_PH66V-1SKW",
        "outputId": "22a6f5d9-3623-4091-cca5-a30ab7f08c58"
      },
      "outputs": [],
      "source": [
        "conf_matrix2 = confusion_matrix(y_true2, y_pred2)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix2, annot=True, cmap='Blues', fmt='g')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG7tF3jZHOli"
      },
      "source": [
        "The classification report and confusion matrix are shown above. These values will be different every single time due to the randomness or variation of the model that is being run.\n",
        "\n",
        "Looking at the classification report, the results from the fine-tuned neural network are much better compared to our neural network in Part B.1. The accuracy consistently increases by about 20%. Also, the f1-scores for each group are much better. Groups 3 and 5 are scoring very high (over 0.85).\n",
        "\n",
        "A lot of the trends seen in the previous neural network are present in the fine-tuned one as well. As discussed previously, group 1 still has a low f1-score, while groups 3 and 5 have the best scores. However, the macro and weighted averages are very similar to each other compared to the other models.\n",
        "\n",
        "Sample size is very small since the entire dataset was only 87 images and the test dataset is only 18 images. This is much smaller than the model in Part B.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07v1D2zjmSud",
        "outputId": "f94d58eb-e793-4adb-80e0-df965ff8904c"
      },
      "outputs": [],
      "source": [
        "print(\"0=angry, 1=disgust, 2=fear, 3=happy, 4=sad, 5=surprise, 6=neutral\")\n",
        "\n",
        "for img in image_paths:\n",
        "  predicted_class = classify_image(img, model_nn)\n",
        "  print(f\"Predicted class for {img}: {predicted_class}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kyhD5i1KLbf"
      },
      "source": [
        "The predicted emotion for each image is printed above. The results of these predictions will be slightly different every time the neural network model is run. However, most of the time there are not too many big fluctuations in the results.\n",
        "\n",
        "The fine-tuned model's predictions are usually much better compared to the predictions of the model in Part B.2. This is due to the fine-tuned model improving its accuracy by about 20%. So, a lot of the predictions are accurate for the fine-tuned model. This model is able to predict about 8-9 of the 12 images correctly. It is really good at predicting happy and sad but struggles with disgust. It typically does not even attempt to predict disgust leading to image 48_01 being predicted wrong the most amount of times. It also sometimes mistakes anger for sadness such as images 4_01 and 26_123.\n",
        "\n",
        "The fine-tuned model usually has an accuracy of about 70% and hits at about that rate when making the predictions on the unknown dataset. It is important to remember that the sample size of the unknown dataset is very small so results can fluctuate by a lot. There are only 12 unknown images, so even if the model predicts just one more image correctly or incorrectly, this would alter the prediction accuracy for the unknown Phoebe images by over 8% (1/12)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrGpboiB-0fA"
      },
      "source": [
        "# Part C - Comparison Between Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7ZZaTOMrxtI"
      },
      "source": [
        "The accuracies of the four models were: SVM-Fer2013 = 0.45, SVM-OpenFace = 0.5, NN-Fer2013 = ~0.5, and NN-FineTuned = ~0.7. The NN-FineTuned model has a much greater accuracy compared to the other 3 models. This accuracy can be seen when predicting the unknown Phoebe image dataset, where is consistently predicts about 8-9 images correctly while the other 3 models predict only about 6 images correctly. All the models were very good at predicting happy and sad emotions but other than NN-FineTuned they struggled with predicting other emotions such as anger and disgust. This can be seen with the f1-scores of every model where they all had their highest scores with the happy and sad groups.\n",
        "\n",
        "The SVM-OpenFace model only predicted three emotions: happy, sad, surprised, while the NN-FineTuned model predicted every emotion other than disgust at least once (excluding fear and neutral since they are not emotions in the Phoebe dataset). Since the SVM-OpenFace model is only predicting a few emotions, we cannot expect it to be very accurate, especially compared to the NN-FineTuned model. The SVM-OpenFace model was very good at predicting happiness as 4 out of its 6 correct predictions are happiness. The NN-FineTuned is able to correctly predict many more emotions consistently such as happy, sad, and surprised. The NN-FineTuned model being trained with the Fer2013 data and then fine-tuned with the Phoebe dataset gives it a big advantage and explains why it is much more accurate and better compared to a model like SVM-OpenFace, that has only been trained with the Phoebe images (less than 100 sample size).\n",
        "\n",
        "The NN-FineTuned worked the best out of the four models. NN-FineTuned is an upgraded version of the NN-Fer2013 model, since it was trained with the Phoebe images as well, so we expect it to perform better and it does. Comparing to the SVM-OpenFace model, NN-FineTuned consistently had about 20% greater accuracy and was able to predict images much better. The SVM-OpenFace model would predict about half the images correctly compared to the NN-FineTuned model, which was able to predict about 70% of the unknown dataset correctly. The NN-FineTuned was also much better than the SVM-Fer2013 model at predicting. It was consistently able to predict about 3-4 more images correctly than the SVM-Fer2013 model. The NN-FineTuned model performed much better than the other 3 models and is the one I would select for this dataset.\n",
        "\n",
        "A big limitation in this assignment was Fer2013 having 7 different emotional labels, while the Phoebe dataset only had 5. This made creating the NN-FineTuned model much more difficult because I had to account for 2 extra emotions that were not available in the Phoebe dataset. This can lead to the model being biased towards certain more frequent emotions and performing poorly on less represented ones. If Fer2013 had the same emotional labels as the Phoebe dataset, we could have potentially seen a more accurate model. Also, since we are going from one dataset to another, features learned from one dataset may not generalize well to another dataset with different characteristics. This is a potential issue other \"emotional recognition\" systems face in the real world since emotions are very subjective and you do not know exactly how certain datas group or interpret certain emotions.\n",
        "\n",
        "Another limitation is the Phoebe dataset is much smaller than the Fer2013 data. When training and predicting model's with the Fer2013 dataset, we know there is a large amount of data, so we do not expect the results to fluctuate by a lot and are more confident with the accuracy scores. Compared to the Phoebe dataset, which has less than 100 images and only about 12 images for predicting. This can potentially lead to high variance in performance across different subsets of data.\n",
        "\n",
        "One more limitation in this assignment is that I have to compare the predicted emotions with how I interpreted Phoebe's emotions from the unknown dataset. This can be very subjective as for example in images 44_01 or 52_31, it was difficult to tell what her emotion were. This can lead to the model's accuracy being different depending on how the emotions are interpreted by someone. Also, you can never tell for sure what emotions someone is feeling just from one picture, which is the biggest challenge \"emotional recognition\" models face."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
